{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "dKQ3BHJPoTEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "id": "3wR09MFQoSTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 transformers datasets sentence-transformers faiss-cpu\n"
      ],
      "metadata": {
        "id": "bmFi_rUakJ6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28\n"
      ],
      "metadata": {
        "id": "u_aIuliGuClD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install unrar -y\n",
        "!pip install rarfile\n"
      ],
      "metadata": {
        "id": "hl0OuH2P0cTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rarfile\n",
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from rank_bm25 import BM25Okapi\n",
        "import re\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "import openai\n",
        "openai.api_key = \"PUTYOURKEYHERE\"\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "AvTg8hMpgJqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# rar_path = \"/content/PHEME_PLUS.rar\"\n",
        "# extract_to = \"/content/PHEME_PLUS\"\n",
        "\n",
        "# # Set up rarfile module\n",
        "# rarfile.UNRAR_TOOL = \"/usr/bin/unrar\"\n",
        "\n",
        "# with rarfile.RarFile(rar_path) as rf:\n",
        "#     rf.extractall(path=extract_to)\n"
      ],
      "metadata": {
        "id": "VSHTQtiQA-Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "\n",
        "def _fetch_text_task(url):\n",
        "    if \"github.com\" in url or \"raw.githubusercontent.com\" in url or url.endswith(\".txt\"):\n",
        "        print(f\"[SKIPPED] Unsupported URL format: {url}\")\n",
        "        return None\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\"\n",
        "    }\n",
        "    response = requests.get(url, timeout=10, headers=headers)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    if \"text/html\" in response.headers.get(\"Content-Type\", \"\"):\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):\n",
        "            tag.decompose()\n",
        "\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text = ' '.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50)\n",
        "        return text.strip() if text else None\n",
        "    else:\n",
        "        print(f\"[SKIPPED] Non-HTML content: {url}\")\n",
        "        return None\n",
        "\n",
        "def fetch_article_text(url, hard_timeout=20):\n",
        "    try:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
        "            future = executor.submit(_fetch_text_task, url)\n",
        "            return future.result(timeout=hard_timeout)\n",
        "    except concurrent.futures.TimeoutError:\n",
        "        print(f\"[TIMEOUT] Skipped (hard timeout): {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Could not fetch or parse article: {url}\\n{e}\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "3qf51yhggH5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_annotation_data(event_path):\n",
        "\n",
        "\n",
        "\n",
        "            annotation_file = os.path.join(event_path, \"annotation.json\")\n",
        "\n",
        "\n",
        "\n",
        "            with open(annotation_file, 'r', encoding='utf-8') as f:\n",
        "                annotation = json.load(f)\n",
        "            print(annotation)\n",
        "\n",
        "            # Determine veracity label\n",
        "            if annotation.get(\"true\") == '1':\n",
        "                veracity = \"TRUE\"\n",
        "            elif annotation.get(\"misinformation\") == '1':\n",
        "                veracity = \"FALSE\"\n",
        "            else:\n",
        "                veracity = \"UNVERIFIED\"\n",
        "\n",
        "            # Get supporting / opposing links\n",
        "            links = annotation.get(\"links\", [])\n",
        "            print(links)\n",
        "            supporting_links = [l[\"link\"] for l in links if l.get(\"position\") == \"for\"]\n",
        "            opposing_links = [l[\"link\"] for l in links if l.get(\"position\") == \"against\"]\n",
        "            return {\n",
        "                \"event\": event_path,\n",
        "\n",
        "                \"claim\": annotation.get(\"category\", \"\"),\n",
        "                \"gold_label\": veracity,\n",
        "                \"supporting_links\": supporting_links,\n",
        "                \"opposing_links\": opposing_links,\n",
        "            }\n",
        "\n"
      ],
      "metadata": {
        "id": "jmquSARuiTCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_source_tweet(event_path):\n",
        "    src_dir = os.path.join(event_path, 'source-tweets')\n",
        "\n",
        "    # Make sure the directory exists\n",
        "    if not os.path.isdir(src_dir):\n",
        "        raise FileNotFoundError(f\"Directory not found: {src_dir}\")\n",
        "\n",
        "    # Find the .json file (there should be only one)\n",
        "    json_files = [f for f in os.listdir(src_dir) if f.endswith('.json')]\n",
        "    if not json_files:\n",
        "        raise FileNotFoundError(f\"No JSON file found in: {src_dir}\")\n",
        "\n",
        "    src_file = os.path.join(src_dir, json_files[0])\n",
        "\n",
        "    # Load and return the tweet text\n",
        "    with open(src_file, 'r', encoding='utf-8') as f:\n",
        "        tweet_data = json.load(f)\n",
        "\n",
        "    return tweet_data.get('text', '')\n"
      ],
      "metadata": {
        "id": "GT6h0MzmnG3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_articles(event_path):\n",
        "    # 1. Load source tweet (assumes one JSON file in source-tweets/)\n",
        "    src_dir = os.path.join(event_path, 'source-tweets')\n",
        "    src_file = next((f for f in os.listdir(src_dir) if f.endswith('.json')), None)\n",
        "    if not src_file:\n",
        "        return []\n",
        "\n",
        "    with open(os.path.join(src_dir, src_file), 'r', encoding='utf-8') as f:\n",
        "        src = json.load(f)\n",
        "    tweet_text = src.get('text', '')\n",
        "    tweet_date = src.get('created_at', '')\n",
        "\n",
        "    # 2. Load article URLs from google/ subfolder\n",
        "    google_dir = os.path.join(event_path, 'google')\n",
        "    if not os.path.exists(google_dir):\n",
        "        return []\n",
        "\n",
        "    article_urls = []\n",
        "    for filename in os.listdir(google_dir):\n",
        "        path = os.path.join(google_dir, filename)\n",
        "        if os.path.isfile(path):\n",
        "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                url = f.read().strip()\n",
        "                if url:\n",
        "                    article_urls.append(url)\n",
        "\n",
        "    # 3. Fetch full article content\n",
        "    full_articles = []\n",
        "    for url in article_urls:\n",
        "        content = fetch_article_text(url)\n",
        "        if content:\n",
        "            full_articles.append({'url': url, 'text': content})\n",
        "\n",
        "    return full_articles\n"
      ],
      "metadata": {
        "id": "0SPVK9EliwvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def extract_sentences(articles):\n",
        "    sentences = []\n",
        "    for art in articles:\n",
        "        for s in nltk.sent_tokenize(art['text']):\n",
        "            if 20 < len(s) < 300:  # filter noise\n",
        "                sentences.append({'text': s, 'source_url': art['url']})\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "MAMTCVy3jqty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_topk_evidence_sentences(claim_text, sentences, k=5):\n",
        "    corpus = [s['text'] for s in sentences]\n",
        "    tokenized = [re.findall(r\"\\w+\", s.lower()) for s in corpus]\n",
        "    bm25 = BM25Okapi(tokenized)\n",
        "    query = re.findall(r\"\\w+\", claim_text.lower())\n",
        "    scores = bm25.get_scores(query)\n",
        "\n",
        "    top_ids = sorted(range(len(scores)), key=lambda i: -scores[i])[:k]\n",
        "    return [sentences[i] for i in top_ids]\n"
      ],
      "metadata": {
        "id": "lqqP6S9ikfjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def nli_logits(premise, hypothesis,tokenizer,nli_model):\n",
        "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        logits = nli_model(**inputs).logits\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    return probs.squeeze().cpu().numpy()  # [entail, neutral, contradict]"
      ],
      "metadata": {
        "id": "IjJ6FwC6rr5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_veracity(claim, evidence_sents,tokenizer,model):\n",
        "    votes = []\n",
        "    for s in evidence_sents:\n",
        "        p = s[\"text\"]\n",
        "        probs = nli_logits(p, claim,tokenizer,model)\n",
        "        cls = probs.argmax()\n",
        "        votes.append(cls)\n",
        "\n",
        "    from collections import Counter\n",
        "    vote_counts = Counter(votes)\n",
        "    final = vote_counts.most_common(1)[0][0]\n",
        "    return [\"SUPPORT\", \"NEI\", \"REFUTE\"][final], vote_counts"
      ],
      "metadata": {
        "id": "_Qg-Te7RrvDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_evidence_only_veracity_prompt(claim, top_evidence):\n",
        "    evidence_str = \"\\n\".join(\n",
        "        [f'- \"{s[\"text\"]}\" (source: {s[\"source_url\"]})' for s in top_evidence]\n",
        "    )\n",
        "\n",
        "    return f\"\"\"\n",
        "You are a rumor verification assistant.\n",
        "\n",
        "Claim: \"{claim}\"\n",
        "\n",
        "Evidence:\n",
        "{evidence_str}\n",
        "\n",
        "Task:\n",
        "Based only on this evidence, classify the claim as one of:\n",
        "- TRUE\n",
        "- FALSE\n",
        "- UNVERIFIED\n",
        "\n",
        "Provide a short justification, and respond in this JSON format:\n",
        "\n",
        "{{\n",
        "  \"veracity\": \"...\",\n",
        "  \"justification\": \"...\"\n",
        "}}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NiCqxvYCzR2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "def parse_tweet_time(timestr):\n",
        "    \"\"\"Parses tweet timestamp format into a datetime object.\"\"\"\n",
        "    return datetime.strptime(timestr, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "\n",
        "def get_time_diff_minutes(t1, t2):\n",
        "    return (t2 - t1).total_seconds() / 60.0\n",
        "\n",
        "def get_first_level_reply_ids(structure, root_id):\n",
        "    return list(structure.get(str(root_id), {}).keys())\n",
        "\n",
        "def classify_stance(claim, reply_text, tokenizer, model):\n",
        "    inputs = tokenizer.encode_plus(claim, reply_text, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()\n",
        "    return [\"entailment\", \"neutral\", \"contradiction\"][predicted_class]\n",
        "\n",
        "def extract_first_level_replies_and_stances(event_folder, tokenizer, model):\n",
        "    try:\n",
        "        # Load structure\n",
        "        with open(os.path.join(event_folder, 'structure.json'), 'r') as f:\n",
        "            structure = json.load(f)\n",
        "\n",
        "        # Load annotation\n",
        "        with open(os.path.join(event_folder, 'annotation.json'), 'r') as f:\n",
        "            ann = json.load(f)\n",
        "        claim = ann.get('claim') or ann.get('category', '')\n",
        "\n",
        "        thread_id = os.path.basename(event_folder.strip('/'))\n",
        "\n",
        "        # Load source tweet\n",
        "        src_dir = os.path.join(event_folder, 'source-tweets')\n",
        "        src_file = next((f for f in os.listdir(src_dir) if f.endswith('.json')), None)\n",
        "        if not src_file:\n",
        "            return None\n",
        "\n",
        "        with open(os.path.join(src_dir, src_file), 'r') as f:\n",
        "            src = json.load(f)\n",
        "        src_time = parse_tweet_time(src[\"created_at\"])\n",
        "\n",
        "        # Get 1st-level replies\n",
        "        reply_ids = get_first_level_reply_ids(structure, thread_id)\n",
        "        stance_data = []\n",
        "\n",
        "        for rid in reply_ids:\n",
        "            tweet_path = os.path.join(event_folder, 'reactions', f'{rid}.json')\n",
        "            if not os.path.exists(tweet_path):\n",
        "                continue\n",
        "            try:\n",
        "                with open(tweet_path, 'r') as f:\n",
        "                    tweet_data = json.load(f)\n",
        "                reply_text = tweet_data.get('text', '')\n",
        "                reply_time = parse_tweet_time(tweet_data.get('created_at', ''))\n",
        "                minutes_since_post = get_time_diff_minutes(src_time, reply_time)\n",
        "\n",
        "                if reply_text:\n",
        "                    stance = classify_stance(claim, reply_text, tokenizer, model)\n",
        "                    stance_data.append({\n",
        "                        \"id\": rid,\n",
        "                        \"text\": reply_text,\n",
        "                        \"stance\": stance,\n",
        "                        \"time_minutes\": round(minutes_since_post, 2)\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                continue  # skip malformed tweet files\n",
        "\n",
        "        return {\n",
        "            \"claim\": claim,\n",
        "            \"reply_stances\": stance_data,\n",
        "            \"reply_ids\": reply_ids\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing thread: {event_folder} | {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "eWyxI-tlut73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_bin_replies_by_time(reply_stances, bin_size=30):\n",
        "    \"\"\"\n",
        "    Groups replies into time bins and counts stance types.\n",
        "\n",
        "    Args:\n",
        "        reply_stances: list of dicts with keys: id, text, stance, time_minutes\n",
        "        bin_size: size of each time bin in minutes (default = 30)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping bin number → stance counts\n",
        "    \"\"\"\n",
        "    binned_counts = defaultdict(lambda: {\"entailment\": 0, \"neutral\": 0, \"contradiction\": 0})\n",
        "\n",
        "    for r in reply_stances:\n",
        "        minutes = r[\"time_minutes\"]\n",
        "        stance = r[\"stance\"]\n",
        "        bin_id = int(minutes // bin_size)  # e.g., 0 for 0–29 min, 1 for 30–59, etc.\n",
        "        binned_counts[bin_id][stance] += 1\n",
        "\n",
        "    return dict(binned_counts)"
      ],
      "metadata": {
        "id": "UhsV2dhqu4Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_reply_only_veracity_prompt(claim, replies, turnaround):\n",
        "    reply_texts = \"\\n\".join([\n",
        "        f'- \"{r[\"text\"]}\" (Stance: {r[\"nli\"]})' for r in replies\n",
        "    ])\n",
        "    turnaround_note = \"Yes\" if turnaround else \"No\"\n",
        "\n",
        "    return f\"\"\"\n",
        "You are a rumor verification assistant analyzing public replies to a tweet.\n",
        "\n",
        "Claim: \"{claim}\"\n",
        "\n",
        "Here are the replies to the tweet, with their inferred stance (via natural language inference):\n",
        "\n",
        "{replies}\n",
        "\n",
        "Has public opinion changed significantly over time (turnaround)? {turnaround_note}\n",
        "\n",
        "Based only on the replies (and the stance + turnaround), assess the likely veracity of the claim. Choose one of:\n",
        "\n",
        "- TRUE: The claim is likely accurate.\n",
        "- FALSE: The claim is likely false.\n",
        "- UNVERIFIED: The replies do not provide sufficient evidence to determine truth.\n",
        "\n",
        "Provide your decision and a brief justification in this format:\n",
        "\n",
        "{{\n",
        "  \"veracity\": \"...\",\n",
        "  \"justification\": \"...\"\n",
        "}}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "7kFqfhSbxepl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_advisor_prompt(claim, response_replies, response_evidence):\n",
        "    return f\"\"\"\n",
        "You are a rumor verification assistant combining two signals to assess the truth of a claim.\n",
        "\n",
        "Claim:\n",
        "\"{claim}\"\n",
        "\n",
        "\n",
        "Reply-based prediction:\n",
        "{response_replies}\n",
        "\n",
        "Evidence-based prediction:\n",
        "{response_evidence}\n",
        "\n",
        "TASK:\n",
        "Choose the final veracity of the claim (TRUE, FALSE, or UNVERIFIED) based on which explanation is more convincing. Also specify which explanation you relied on more.\n",
        "\n",
        "Respond in JSON format:\n",
        "\n",
        "{{\n",
        "  \"final_veracity\": \"...\",\n",
        "  \"preferred_explanation\": \"reply\" or \"evidence\",\n",
        "  \"justification\": \"...\"\n",
        "}}\"\"\""
      ],
      "metadata": {
        "id": "wl_OErWy0rW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "def detect_turnaround(reply_bins, turnaround_window=2):\n",
        "    stances_over_time = []\n",
        "    sorted_bins = sorted(reply_bins.items())\n",
        "\n",
        "    for _, stance_counts in sorted_bins:\n",
        "        if sum(stance_counts.values()) == 0:\n",
        "            continue  # skip empty bins\n",
        "        majority_stance = max(stance_counts, key=stance_counts.get)\n",
        "        stances_over_time.append(majority_stance)\n",
        "\n",
        "    if len(stances_over_time) < turnaround_window + 1:\n",
        "        return stances_over_time, False  # Not enough data to judge\n",
        "\n",
        "    # Compute turnaround\n",
        "    recent_stances = stances_over_time[-turnaround_window:]\n",
        "    recent_contradictions = sum(1 for s in recent_stances if s == \"contradiction\")\n",
        "\n",
        "    early_stances = stances_over_time[:-turnaround_window]\n",
        "    if not early_stances:\n",
        "        return stances_over_time, False  # Avoid division by zero\n",
        "\n",
        "    early_entailments = sum(1 for s in early_stances if s == \"entailment\")\n",
        "    turnaround_detected = (\n",
        "        recent_contradictions >= turnaround_window // 2 + 1 and\n",
        "        early_entailments >= max(1, len(early_stances) // 2)\n",
        "    )\n",
        "\n",
        "    return stances_over_time, turnaround_detected\n"
      ],
      "metadata": {
        "id": "q0s742iR5AbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_veracity_info(reply_json_str, evidence_json_str, advisor_json_str):\n",
        "    # Load each JSON string\n",
        "    reply = json.loads(reply_json_str)\n",
        "    evidence = json.loads(evidence_json_str)\n",
        "    advisor = json.loads(advisor_json_str)\n",
        "\n",
        "    # Extract fields\n",
        "    return {\n",
        "        \"reply_veracity\": reply.get(\"veracity\"),\n",
        "        \"reply_explanation\": reply.get(\"justification\"),\n",
        "        \"evidence_veracity\": evidence.get(\"veracity\"),\n",
        "        \"evidence_explanation\": evidence.get(\"justification\"),\n",
        "        \"preferred_explanation\": advisor.get(\"preferred_explanation\"),\n",
        "        \"final_veracity\": advisor.get(\"final_veracity\"),\n",
        "        \"final_explanation\": advisor.get(\"justification\"),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "xFXhxSUbBWrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# def run_pipeline(base_path):\n",
        "\n",
        "\n",
        "#     for event_folder in os.listdir(base_path):\n",
        "#         event_path = os.path.join(base_path, event_folder, \"rumours\")\n",
        "#         if not os.path.isdir(event_path):\n",
        "#             continue\n",
        "\n",
        "#         print(f\"Processing event: {event_folder}\")\n",
        "#         for rumour_id in os.listdir(event_path):\n",
        "#           event_path+=f'/{rumour_id}'\n",
        "#           annotation_data=extract_annotation_data(event_path)\n",
        "#           source_tweet=extract_source_tweet(event_path)\n",
        "#           articles=extract_articles(event_path)\n",
        "#           sentences=extract_sentences(articles)\n",
        "#           topk_evidence_sentences=extract_topk_evidence_sentences(annotation_data['claim'],sentences, k=5)\n",
        "#           evidence_prompt=build_evidence_only_veracity_prompt(annotation_data['claim'], topk_evidence_sentences)\n",
        "#           response_evidence= openai.ChatCompletion.create(\n",
        "#             model=\"gpt-3.5-turbo\",\n",
        "#             messages=[{\"role\": \"user\", \"content\": evidence_prompt}],\n",
        "#             temperature=0,\n",
        "#           )\n",
        "\n",
        "#           print(response_evidence['choices'][0]['message']['content'])\n",
        "#           tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
        "#           model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").eval()\n",
        "#           first_level_replies_and_stances=extract_first_level_replies_and_stances(event_path,tokenizer,model)\n",
        "#           reply_bins=extract_bin_replies_by_time(first_level_replies_and_stances['reply_stances'])\n",
        "#           seq,turnaround=detect_turnaround(reply_bins)\n",
        "#           replies = [\n",
        "#           {\"text\": r[\"text\"], \"nli\": r[\"stance\"]}\n",
        "#           for r in first_level_replies_and_stances['reply_stances']\n",
        "#           ]\n",
        "#           replies_prompt=build_reply_only_veracity_prompt(annotation_data['claim'], replies, turnaround)\n",
        "#           response_replies = openai.ChatCompletion.create(\n",
        "#             model=\"gpt-3.5-turbo\",\n",
        "#             messages=[{\"role\": \"user\", \"content\": replies_prompt}],\n",
        "#             temperature=0,\n",
        "#           )\n",
        "\n",
        "#           print(response_replies['choices'][0]['message']['content'])\n",
        "#           advisor_prompt = build_advisor_prompt(annotation_data['claim'],response_replies,response_evidence)\n",
        "#           response = openai.ChatCompletion.create(\n",
        "#           model=\"gpt-3.5-turbo\",\n",
        "#           messages=[{\"role\": \"user\", \"content\": advisor_prompt}],\n",
        "#           temperature=0,\n",
        "#           )\n",
        "#           veracity_info= extract_veracity_info(response_replies, response_evidence, response)\n",
        "\n",
        "#           print(response['choices'][0]['message']['content'])\n",
        "#           print(\"EVENT PATH:\",event_folder)\n",
        "#           print(\"RUMOUR ID:\",rumour_id)\n",
        "#           print(\"GOLD LABEL:\",annotation_data['gold_label'])\n",
        "#           print(\"CLAIM:\",annotation_data['claim'])\n",
        "#           for k, v in veracity_info.items():\n",
        "#               print(f\"{k}: {v}\")\n",
        "#           add_to_csv()\n",
        "\n",
        "#         #   break;\n",
        "#         # break;"
      ],
      "metadata": {
        "id": "t34CL386Ew6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n"
      ],
      "metadata": {
        "id": "L0zpAHX2_LBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import csv\n",
        "\n",
        "# def run_pipeline(base_path, output_csv=\"veracity_pipeline_results.csv\"):\n",
        "#     # Define CSV column names\n",
        "#     fieldnames = [\n",
        "#         \"event_path\",\n",
        "#         \"rumour_id\",\n",
        "#         \"gold_label\",\n",
        "#         \"claim\",\n",
        "#         \"turnaround\",\n",
        "#         \"reply_veracity\",\n",
        "#         \"reply_explanation\",\n",
        "#         \"evidence_veracity\",\n",
        "#         \"evidence_explanation\",\n",
        "#         \"preferred_explanation\",\n",
        "#         \"final_veracity\",\n",
        "#         \"final_explanation\"\n",
        "#     ]\n",
        "\n",
        "#     # Open CSV file\n",
        "#     with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "#         writer.writeheader()\n",
        "\n",
        "#         # Begin pipeline\n",
        "#         for event_folder in os.listdir(base_path):\n",
        "#             event_path = os.path.join(base_path, event_folder, \"rumours\")\n",
        "#             if not os.path.isdir(event_path):\n",
        "#                 continue\n",
        "\n",
        "#             print(f\"Processing event: {event_folder}\")\n",
        "#             for rumour_id in os.listdir(event_path):\n",
        "#                 thread_path = os.path.join(event_path, rumour_id)\n",
        "#                 try:\n",
        "#                     annotation_data = extract_annotation_data(thread_path)\n",
        "#                     source_tweet = extract_source_tweet(thread_path)\n",
        "#                     articles = extract_articles(thread_path)\n",
        "#                     sentences = extract_sentences(articles)\n",
        "#                     topk_evidence_sentences = extract_topk_evidence_sentences(\n",
        "#                         annotation_data['claim'], sentences, k=5\n",
        "#                     )\n",
        "\n",
        "#                     evidence_prompt = build_evidence_only_veracity_prompt(\n",
        "#                         annotation_data['claim'], topk_evidence_sentences\n",
        "#                     )\n",
        "#                     response_evidence = openai.ChatCompletion.create(\n",
        "#                         model=\"gpt-3.5-turbo\",\n",
        "#                         messages=[{\"role\": \"user\", \"content\": evidence_prompt}],\n",
        "#                         temperature=0,\n",
        "#                     )\n",
        "\n",
        "#                     tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
        "#                     model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").eval()\n",
        "\n",
        "#                     first_level_replies_and_stances = extract_first_level_replies_and_stances(\n",
        "#                         thread_path, tokenizer, model\n",
        "#                     )\n",
        "#                     reply_bins = extract_bin_replies_by_time(first_level_replies_and_stances['reply_stances'])\n",
        "#                     seq, turnaround = detect_turnaround(reply_bins)\n",
        "#                     replies = [\n",
        "#                         {\"text\": r[\"text\"], \"nli\": r[\"stance\"]}\n",
        "#                         for r in first_level_replies_and_stances['reply_stances']\n",
        "#                     ]\n",
        "#                     replies_prompt = build_reply_only_veracity_prompt(\n",
        "#                         annotation_data['claim'], replies, turnaround\n",
        "#                     )\n",
        "#                     response_replies = openai.ChatCompletion.create(\n",
        "#                         model=\"gpt-3.5-turbo\",\n",
        "#                         messages=[{\"role\": \"user\", \"content\": replies_prompt}],\n",
        "#                         temperature=0,\n",
        "#                     )\n",
        "\n",
        "#                     advisor_prompt = build_advisor_prompt(\n",
        "#                         annotation_data['claim'], response_replies, response_evidence\n",
        "#                     )\n",
        "#                     response_advisor = openai.ChatCompletion.create(\n",
        "#                         model=\"gpt-3.5-turbo\",\n",
        "#                         messages=[{\"role\": \"user\", \"content\": advisor_prompt}],\n",
        "#                         temperature=0,\n",
        "#                     )\n",
        "\n",
        "#                     veracity_info = extract_veracity_info(\n",
        "#                         response_replies['choices'][0]['message']['content'],\n",
        "#                         response_evidence['choices'][0]['message']['content'],\n",
        "#                         response_advisor['choices'][0]['message']['content'],\n",
        "#                     )\n",
        "\n",
        "#                     # Write row to CSV\n",
        "#                     writer.writerow({\n",
        "#                         \"event_path\": event_folder,\n",
        "#                         \"rumour_id\": rumour_id,\n",
        "#                         \"gold_label\": annotation_data.get('gold_label'),\n",
        "#                         \"claim\": annotation_data.get('claim'),\n",
        "#                         \"turnaround\":turnaround,\n",
        "#                         **veracity_info\n",
        "#                     })\n",
        "\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"❌ Error processing {event_folder}/{rumour_id}: {e}\")\n",
        "#                     continue\n",
        "#     # After all processing is done:\n",
        "#     print(f\"✅ Finished! Downloading CSV: {output_csv}\")\n",
        "#     files.download(output_csv)\n",
        "\n"
      ],
      "metadata": {
        "id": "GKKPk7wIDMuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run_pipeline('/content/PHEME_PLUS/all-rnr-annotated-threads',)"
      ],
      "metadata": {
        "id": "s_rXFU6-5CQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, csv\n",
        "from collections import defaultdict\n",
        "\n",
        "def save_prompt_components_to_csv(base_path, output_file=\"prompt_inputs.csv\"):\n",
        "    fieldnames = [\n",
        "        \"event\", \"rumour_id\", \"claim\", \"gold_label\", \"source_tweet\",\n",
        "        \"reply_stances_json\", \"stance_bins_json\", \"turnaround\",\n",
        "        \"evidence_urls\", \"top_evidence_sentences\"\n",
        "    ]\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for event in os.listdir(base_path):\n",
        "            rumours_path = os.path.join(base_path, event, \"rumours\")\n",
        "            if not os.path.isdir(rumours_path): continue\n",
        "\n",
        "            for rumour_id in os.listdir(rumours_path):\n",
        "                thread_path = os.path.join(rumours_path, rumour_id)\n",
        "                try:\n",
        "                    ann = extract_annotation_data(thread_path)\n",
        "                    source = extract_source_tweet(thread_path)\n",
        "                    articles = extract_articles(thread_path)\n",
        "                    article_urls = [a[\"url\"] for a in articles]\n",
        "\n",
        "                    # Evidence component\n",
        "                    all_sentences = extract_sentences(articles)\n",
        "                    top_sents = extract_topk_evidence_sentences(ann['claim'], all_sentences, k=5)\n",
        "\n",
        "                    # Reply component\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
        "                    model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").eval()\n",
        "                    reply_data = extract_first_level_replies_and_stances(thread_path, tokenizer, model)\n",
        "                    bins = extract_bin_replies_by_time(reply_data[\"reply_stances\"])\n",
        "                    _, turnaround = detect_turnaround(bins)\n",
        "\n",
        "                    writer.writerow({\n",
        "                        \"event\": event,\n",
        "                        \"rumour_id\": rumour_id,\n",
        "                        \"claim\": ann.get(\"claim\") or ann.get(\"category\", \"\"),\n",
        "                        \"gold_label\": ann.get(\"true\", 0) and \"TRUE\" or (ann.get(\"misinformation\", 0) and \"FALSE\" or \"UNVERIFIED\"),\n",
        "                        \"source_tweet\": source,\n",
        "                        \"reply_stances_json\": json.dumps(reply_data[\"reply_stances\"]),\n",
        "                        \"stance_bins_json\": json.dumps(bins),\n",
        "                        \"turnaround\": turnaround,\n",
        "                        \"evidence_urls\": json.dumps(article_urls),\n",
        "                        \"top_evidence_sentences\": json.dumps([s[\"text\"] for s in top_sents])\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing {event}/{rumour_id}: {e}\")\n",
        "\n",
        "    print(f\"✅ Data saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "rQ0vBg65f37a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_prompt_components_to_csv('/content/PHEME_PLUS/all-rnr-annotated-threads',)"
      ],
      "metadata": {
        "id": "aMJ0ebX2f9pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYikE9u0gIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"prompt_inputs.csv\")\n"
      ],
      "metadata": {
        "id": "_XwV0SyOWisG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load CSV\n",
        "csv_path = \"prompt_inputs.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Ensure rumour_id is int to match filesystem\n",
        "df[\"rumour_id\"] = df[\"rumour_id\"].astype(int)\n",
        "df.set_index([\"event\", \"rumour_id\"], inplace=True)\n",
        "\n",
        "# Path to PHEME_PLUS annotation folders\n",
        "base_path = \"/content/PHEME_PLUS/all-rnr-annotated-threads\"\n",
        "\n",
        "# Track statistics\n",
        "updated_count = 0\n",
        "total_annotation_threads = 0\n",
        "\n",
        "for event in os.listdir(base_path):\n",
        "    event_path = os.path.join(base_path, event, \"rumours\")\n",
        "    if not os.path.isdir(event_path):\n",
        "        continue\n",
        "\n",
        "    for rumour_id in os.listdir(event_path):\n",
        "        total_annotation_threads += 1\n",
        "        try:\n",
        "            rumour_id_int = int(rumour_id)\n",
        "            annotation_path = os.path.join(event_path, rumour_id, \"annotation.json\")\n",
        "            if not os.path.exists(annotation_path):\n",
        "                continue\n",
        "\n",
        "            with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                ann = json.load(f)\n",
        "\n",
        "            # Match claim only if row exists in CSV\n",
        "            if (event, rumour_id_int) in df.index:\n",
        "                csv_claim = str(df.loc[(event, rumour_id_int), \"claim\"]).strip()\n",
        "                ann_claim = str(ann.get(\"category\", \"\")).strip()\n",
        "\n",
        "                if csv_claim == ann_claim:\n",
        "                    if str(ann.get(\"true\")) == '1':\n",
        "                        df.loc[(event, rumour_id_int), \"gold_label\"] = \"TRUE\"\n",
        "                    elif str(ann.get(\"misinformation\")) == '1':\n",
        "                        df.loc[(event, rumour_id_int), \"gold_label\"] = \"FALSE\"\n",
        "                    else:\n",
        "                        df.loc[(event, rumour_id_int), \"gold_label\"] = \"UNVERIFIED\"\n",
        "                    updated_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error processing {event}/{rumour_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "# Save updated CSV\n",
        "df.reset_index(inplace=True)\n",
        "df.to_csv(\"prompt_inputs_gold_updated.csv\", index=False)\n",
        "\n",
        "# Print statistics\n",
        "print(f\"\\n✅ Updated {updated_count} gold_label entries based on matching claims.\")\n",
        "print(f\"📦 Total rumours in directory: {total_annotation_threads}\")\n",
        "print(f\"🗂️  Total entries in CSV: {len(df)}\")\n",
        "print(\"📄 Saved updated file as 'prompt_inputs_gold_updated.csv'\")\n"
      ],
      "metadata": {
        "id": "iSGX7XAtkEs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ZbmDJ_Q9kHJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_ids = []\n",
        "\n",
        "# Reload your CSV with index\n",
        "df = pd.read_csv(\"prompt_inputs.csv\")\n",
        "df[\"rumour_id\"] = df[\"rumour_id\"].astype(int)\n",
        "df.set_index([\"event\", \"rumour_id\"], inplace=True)\n",
        "\n",
        "base_path = \"/content/PHEME_PLUS/all-rnr-annotated-threads\"\n",
        "\n",
        "for event in os.listdir(base_path):\n",
        "    event_path = os.path.join(base_path, event, \"rumours\")\n",
        "    if not os.path.isdir(event_path):\n",
        "        continue\n",
        "\n",
        "    for rumour_id in os.listdir(event_path):\n",
        "        try:\n",
        "            rumour_id_int = int(rumour_id)\n",
        "            if (event, rumour_id_int) not in df.index:\n",
        "                missing_ids.append((event, rumour_id))\n",
        "        except ValueError:\n",
        "            continue  # Skip folders that are not numeric IDs\n",
        "\n",
        "# Print results\n",
        "print(f\"❌ Total missing rumours not in CSV: {len(missing_ids)}\\n\")\n",
        "for event, rid in missing_ids:\n",
        "    print(f\"{event}/{rid}\")\n"
      ],
      "metadata": {
        "id": "3GZP0ozamcnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Define paths\n",
        "base_path = \"/content/PHEME_PLUS/all-rnr-annotated-threads\"\n",
        "csv_path = \"prompt_inputs_gold_updated.csv\"\n",
        "\n",
        "# Load existing CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "existing_keys = set(zip(df['event'], df['rumour_id'].astype(str)))\n",
        "\n",
        "# Prepare to append\n",
        "fieldnames = [\n",
        "    \"event\", \"rumour_id\", \"claim\", \"gold_label\", \"source_tweet\",\n",
        "    \"reply_stances_json\", \"stance_bins_json\", \"turnaround\",\n",
        "    \"evidence_urls\", \"top_evidence_sentences\"\n",
        "]\n",
        "\n",
        "# Load tokenizer/model once\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").eval()\n",
        "\n",
        "# Reopen CSV in append mode\n",
        "with open(csv_path, 'a', encoding='utf-8', newline='') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "\n",
        "    for event in os.listdir(base_path):\n",
        "        event_rumour_path = os.path.join(base_path, event, \"rumours\")\n",
        "        if not os.path.isdir(event_rumour_path):\n",
        "            continue\n",
        "\n",
        "        for rumour_id in os.listdir(event_rumour_path):\n",
        "            if (event, rumour_id) in existing_keys:\n",
        "                continue\n",
        "\n",
        "            thread_path = os.path.join(event_rumour_path, rumour_id)\n",
        "\n",
        "            try:\n",
        "                # 1. Annotation\n",
        "                ann = extract_annotation_data(thread_path)\n",
        "                claim = ann.get(\"claim\") or ann.get(\"category\", \"\") or \"UNKNOWN_CLAIM\"\n",
        "\n",
        "                # 2. Source tweet\n",
        "                try:\n",
        "                    source_tweet = extract_source_tweet(thread_path)\n",
        "                except:\n",
        "                    source_tweet = \"\"\n",
        "\n",
        "                # 3. Evidence\n",
        "                try:\n",
        "                    articles = extract_articles(thread_path)\n",
        "                    article_urls = [a[\"url\"] for a in articles if \"url\" in a]\n",
        "                    all_sentences = extract_sentences(articles)\n",
        "                    top_sents = extract_topk_evidence_sentences(claim, all_sentences, k=5)\n",
        "                    top_sents_safe = [s[\"text\"] for s in top_sents if isinstance(s, dict) and \"text\" in s and s[\"text\"].strip()]\n",
        "                except Exception as e:\n",
        "                    article_urls = []\n",
        "                    top_sents_safe = []\n",
        "                    print(f\"⚠️ Evidence fallback for {event}/{rumour_id}: {e}\")\n",
        "\n",
        "                # 4. Replies\n",
        "                try:\n",
        "                    reply_data = extract_first_level_replies_and_stances(thread_path, tokenizer, model)\n",
        "                    bins = extract_bin_replies_by_time(reply_data.get(\"reply_stances\", []))\n",
        "                    _, turnaround = detect_turnaround(bins)\n",
        "                except Exception as e:\n",
        "                    reply_data = {\"reply_stances\": []}\n",
        "                    bins = {}\n",
        "                    turnaround = False\n",
        "                    print(f\"⚠️ Reply fallback for {event}/{rumour_id}: {e}\")\n",
        "\n",
        "                # 5. Label\n",
        "                if str(ann.get(\"true\")) == \"1\":\n",
        "                    label = \"TRUE\"\n",
        "                elif str(ann.get(\"misinformation\")) == \"1\":\n",
        "                    label = \"FALSE\"\n",
        "                else:\n",
        "                    label = \"UNVERIFIED\"\n",
        "\n",
        "                # 6. Write row\n",
        "                writer.writerow({\n",
        "                    \"event\": event,\n",
        "                    \"rumour_id\": rumour_id,\n",
        "                    \"claim\": claim,\n",
        "                    \"gold_label\": label,\n",
        "                    \"source_tweet\": source_tweet,\n",
        "                    \"reply_stances_json\": json.dumps(reply_data[\"reply_stances\"]),\n",
        "                    \"stance_bins_json\": json.dumps(bins),\n",
        "                    \"turnaround\": turnaround,\n",
        "                    \"evidence_urls\": json.dumps(article_urls),\n",
        "                    \"top_evidence_sentences\": json.dumps(top_sents_safe)\n",
        "                })\n",
        "                print(f\"✅ Added: {event}/{rumour_id}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Unexpected error at {event}/{rumour_id}: {e}\")\n",
        "                continue\n"
      ],
      "metadata": {
        "id": "qN7viC3VsWXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"prompt_inputs_gold_updated.csv\")"
      ],
      "metadata": {
        "id": "Evt1NyVptM42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Define paths\n",
        "\n",
        "csv_path = \"/content/prompt_inputs_gold_updated (3).csv\"\n",
        "\n",
        "# Load existing CSV\n",
        "df = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "3P_0oO4MxDFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total entries in CSV: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "jCw11uKTBnBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "unique_rumour_ids = df[\"rumour_id\"].unique()\n",
        "print(f\"Total unique rumour IDs: {len(unique_rumour_ids)}\")\n",
        "\n",
        "# Optionally, print the actual IDs\n",
        "print(\"Unique rumour IDs:\")\n",
        "for rid in unique_rumour_ids:\n",
        "    print(rid)\n"
      ],
      "metadata": {
        "id": "HyKQbYV-Buk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "turnaround_true_df = df[df[\"turnaround\"] == True]\n",
        "\n",
        "# Print the number of such entries and preview\n",
        "print(f\"Total entries with turnaround = True: {len(turnaround_true_df)}\\n\")\n",
        "print(turnaround_true_df[[\"event\", \"rumour_id\", \"claim\", \"turnaround\"]])\n"
      ],
      "metadata": {
        "id": "D4oTBd9rB9Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "\n",
        "\n",
        "# Parse the reply_stances_json column from string to list\n",
        "df[\"reply_stances_json\"] = df[\"reply_stances_json\"].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
        "\n",
        "# Filter entries with empty replies\n",
        "empty_replies_df = df[df[\"reply_stances_json\"].apply(lambda x: isinstance(x, list) and len(x) == 0)]\n",
        "\n",
        "# Print count and some details\n",
        "print(f\"Total entries with empty replies: {len(empty_replies_df)}\")\n",
        "print(empty_replies_df[[\"event\", \"rumour_id\", \"claim\", \"reply_stances_json\"]])\n"
      ],
      "metadata": {
        "id": "QgXBUb_uCPBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_eval(val):\n",
        "    if isinstance(val, list):\n",
        "        return val\n",
        "    if isinstance(val, str) and val.strip():\n",
        "        try:\n",
        "            return ast.literal_eval(val)\n",
        "        except Exception:\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "# Safely parse the JSON-like strings into lists\n",
        "\n",
        "df[\"top_evidence_sentences\"] = df[\"top_evidence_sentences\"].apply(safe_eval)\n",
        "\n",
        "# Find entries where both are empty\n",
        "empty_evidence_df = df[\n",
        "\n",
        "    df[\"top_evidence_sentences\"].apply(lambda x: isinstance(x, list) and len(x) == 0)\n",
        "]\n",
        "\n",
        "# Display results\n",
        "print(f\"🔍 Total entries with empty evidence: {len(empty_evidence_df)}\")\n",
        "print(empty_both_df[[\"event\", \"rumour_id\", \"claim\", \"gold_label\"]])"
      ],
      "metadata": {
        "id": "IZUDsHEWDwfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "\n",
        "\n",
        "def safe_eval(val):\n",
        "    if isinstance(val, list):\n",
        "        return val\n",
        "    if isinstance(val, str) and val.strip():\n",
        "        try:\n",
        "            return ast.literal_eval(val)\n",
        "        except Exception:\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "# Safely parse the JSON-like strings into lists\n",
        "df[\"reply_stances_json\"] = df[\"reply_stances_json\"].apply(safe_eval)\n",
        "df[\"top_evidence_sentences\"] = df[\"top_evidence_sentences\"].apply(safe_eval)\n",
        "\n",
        "# Find entries where both are empty\n",
        "empty_both_df = df[\n",
        "    df[\"reply_stances_json\"].apply(lambda x: isinstance(x, list) and len(x) == 0) &\n",
        "    df[\"top_evidence_sentences\"].apply(lambda x: isinstance(x, list) and len(x) == 0)\n",
        "]\n",
        "\n",
        "# Display results\n",
        "print(f\"🔍 Total entries with both empty replies and evidence: {len(empty_both_df)}\")\n",
        "print(empty_both_df[[\"event\", \"rumour_id\", \"claim\", \"gold_label\"]])\n"
      ],
      "metadata": {
        "id": "vJ6BdRftCofC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Removes surrogate pairs, control chars, non-printables.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove surrogate pairs\n",
        "    cleaned = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
        "    # Remove invisible control characters (non-ASCII)\n",
        "    cleaned = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', cleaned)\n",
        "    # Remove other non-printable unicode (optional)\n",
        "    cleaned = re.sub(r'[^\\x20-\\x7E\\u00A0-\\uFFFF]', '', cleaned)\n",
        "    return cleaned.strip()\n",
        "\n",
        "def clean_reply_texts(reply_json_str):\n",
        "    \"\"\"Cleans each reply text in the reply JSON list.\"\"\"\n",
        "    try:\n",
        "        replies = ast.literal_eval(reply_json_str)\n",
        "        for r in replies:\n",
        "            r[\"text\"] = clean_text(r.get(\"text\", \"\"))\n",
        "        return json.dumps(replies)\n",
        "    except Exception:\n",
        "        return \"[]\"\n",
        "\n",
        "def clean_evidence_text(evidence_json_str):\n",
        "    \"\"\"Cleans list of evidence sentences.\"\"\"\n",
        "    try:\n",
        "        sentences = ast.literal_eval(evidence_json_str)\n",
        "        return json.dumps([clean_text(s) for s in sentences])\n",
        "    except Exception:\n",
        "        return \"[]\"\n",
        "\n",
        "# ✅ Load your CSV\n",
        "df = pd.read_csv(\"/content/prompt_inputs_gold_updated (3).csv\")\n",
        "\n",
        "# ✅ Clean individual fields\n",
        "df[\"claim\"] = df[\"claim\"].apply(clean_text)\n",
        "df[\"source_tweet\"] = df[\"source_tweet\"].apply(clean_text)\n",
        "df[\"reply_stances_json\"] = df[\"reply_stances_json\"].apply(lambda x: clean_reply_texts(x) if pd.notna(x) else \"[]\")\n",
        "df[\"top_evidence_sentences\"] = df[\"top_evidence_sentences\"].apply(lambda x: clean_evidence_text(x) if pd.notna(x) else \"[]\")\n",
        "\n",
        "# ✅ Save the cleaned version\n",
        "df.to_csv(\"prompt_inputs_cleaned.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ All problematic characters removed and saved to 'prompt_inputs_cleaned.csv'\")\n"
      ],
      "metadata": {
        "id": "V3KIJwaWnqwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Small, fast, decent performance\n",
        "\n",
        "def filter_relevant_sentences(claim, sentences, threshold=0.5):\n",
        "    if not sentences:\n",
        "        return []\n",
        "    embeddings = model.encode([claim] + sentences, convert_to_tensor=True)\n",
        "    claim_emb = embeddings[0]\n",
        "    sentence_embs = embeddings[1:]\n",
        "    cosine_scores = util.cos_sim(claim_emb, sentence_embs)\n",
        "    return [s for s, score in zip(sentences, cosine_scores[0]) if score >= threshold]\n"
      ],
      "metadata": {
        "id": "Mvdu97zfDkpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import ast\n",
        "# import json\n",
        "\n",
        "# def build_and_attach_gpt_prompts(csv_path, output_path=\"with_prompts.csv\"):\n",
        "#     df = pd.read_csv(csv_path)\n",
        "\n",
        "#     prompt_texts = []\n",
        "\n",
        "#     for _, row in df.iterrows():\n",
        "#         # Parse fields\n",
        "#         try:\n",
        "#             replies = ast.literal_eval(row[\"reply_stances_json\"]) if pd.notna(row[\"reply_stances_json\"]) else []\n",
        "#             evidence = ast.literal_eval(row[\"top_evidence_sentences\"]) if pd.notna(row[\"top_evidence_sentences\"]) else []\n",
        "#         except Exception as e:\n",
        "#             print(f\"Skipping malformed row due to parsing error: {e}\")\n",
        "#             prompt_texts.append(\"\")  # Keep index alignment\n",
        "#             continue\n",
        "\n",
        "#         # System prompt\n",
        "#         system_prompt = (\n",
        "#             \"You are a rumor verification assistant. You will receive a claim, replies from users, and supporting news evidence.\\n\"\n",
        "#             \"Your job is to classify the claim as one of: TRUE, FALSE, or UNVERIFIED, and provide a brief justification.\\n\"\n",
        "#             \"You must consider both the replies (social context) and news evidence (external context).\"\n",
        "#         )\n",
        "\n",
        "#         # Replies\n",
        "#         if replies:\n",
        "#             reply_section = \"\\n\".join([\n",
        "#                 f'- \"{r.get(\"text\", \"\").strip()}\" (Stance: {r.get(\"stance\", \"unknown\")})' for r in replies\n",
        "#             ])\n",
        "#         else:\n",
        "#             reply_section = \"No replies available.\"\n",
        "\n",
        "#         # Evidence\n",
        "#         if evidence:\n",
        "#             evidence_section = \"\\n\".join([\n",
        "#                 f'- \"{e.strip()}\"' for e in evidence if e.strip()\n",
        "#             ])\n",
        "#         else:\n",
        "#             evidence_section = \"No evidence available.\"\n",
        "\n",
        "#         # Instruction\n",
        "#         instruction_prompt = f\"\"\"\n",
        "# Claim:\n",
        "# \"{row['claim']}\"\n",
        "\n",
        "# Replies:\n",
        "# {reply_section}\n",
        "\n",
        "# Evidence:\n",
        "# {evidence_section}\n",
        "\n",
        "# Turnaround detected? {row['turnaround']}\n",
        "\n",
        "# Now, based on both replies and evidence, classify the claim as one of:\n",
        "# - TRUE\n",
        "# - FALSE\n",
        "# - UNVERIFIED\n",
        "\n",
        "# Return the result in this format:\n",
        "# {{\n",
        "#   \"veracity\": \"...\",\n",
        "#   \"justification\": \"...\"\n",
        "# }}\n",
        "# \"\"\".strip()\n",
        "\n",
        "#         # Combine system and user message into a single prompt for record\n",
        "#         combined_prompt = {\n",
        "#             \"system\": system_prompt,\n",
        "#             \"user\": instruction_prompt\n",
        "#         }\n",
        "\n",
        "#         prompt_texts.append(json.dumps(combined_prompt, ensure_ascii=False))\n",
        "\n",
        "#     df[\"consolidated_prompt\"] = prompt_texts\n",
        "#     df.to_csv(\"output_with_prompts.csv\", index=False, encoding=\"utf-8\", errors=\"replace\")\n",
        "\n",
        "\n",
        "#     print(f\"✅ Prompts added and saved to: {output_path}\")\n",
        "\n",
        "#     return df\n"
      ],
      "metadata": {
        "id": "2s9JEF9uC_IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import ast\n",
        "# import json\n",
        "# from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# # Load embedding model\n",
        "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "\n",
        "# def build_and_attach_filtered_prompts(csv_path, output_path=\"output_with_filtered_prompts.csv\"):\n",
        "#     df = pd.read_csv(csv_path)\n",
        "#     prompt_texts = []\n",
        "\n",
        "#     for _, row in df.iterrows():\n",
        "#         try:\n",
        "#             claim = str(row[\"claim\"])\n",
        "#             turnaround = str(row.get(\"turnaround\", \"unknown\"))\n",
        "\n",
        "#             replies = ast.literal_eval(row[\"reply_stances_json\"]) if pd.notna(row[\"reply_stances_json\"]) else []\n",
        "#             evidence = ast.literal_eval(row[\"top_evidence_sentences\"]) if pd.notna(row[\"top_evidence_sentences\"]) else []\n",
        "\n",
        "#             reply_texts = [r.get(\"text\", \"\") for r in replies]\n",
        "#             relevant_replies = replies\n",
        "#             relevant_evidence = evidence\n",
        "\n",
        "#             if relevant_replies:\n",
        "#                 formatted_replies = \"\\n\".join([\n",
        "#                     f'- \"{r.strip()}\" (Stance: {replies[i].get(\"stance\", \"unknown\")})'\n",
        "#                     for i, r in enumerate(reply_texts) if r.strip() in relevant_replies\n",
        "#                 ])\n",
        "#             else:\n",
        "#                 formatted_replies = \"No relevant replies found.\"\n",
        "\n",
        "#             if relevant_evidence:\n",
        "#                 formatted_evidence = \"\\n\".join([f'- \"{e.strip()}\"' for e in relevant_evidence])\n",
        "#             else:\n",
        "#                 formatted_evidence = \"No relevant evidence found.\"\n",
        "\n",
        "#             if relevant_evidence and not relevant_replies:\n",
        "#                 system_prompt = (\n",
        "#                     \"You are a rumor verification assistant. You will receive a claim and news evidence.\\n\"\n",
        "#                     \"Classify the claim as TRUE, FALSE, or UNVERIFIED based on the evidence and explain briefly.\"\n",
        "#                 )\n",
        "#             elif relevant_replies and not relevant_evidence:\n",
        "#                 system_prompt = (\n",
        "#                     \"You are a rumor verification assistant. You will receive a claim and replies from users.\\n\"\n",
        "#                     \"Classify the claim as TRUE, FALSE, or UNVERIFIED based on the replies and explain briefly.\"\n",
        "#                 )\n",
        "#             else:\n",
        "#                 system_prompt = (\n",
        "#                     \"You are a rumor verification assistant. You will receive a claim, replies from users, and supporting news evidence.\\n\"\n",
        "#                     \"Classify the claim as TRUE, FALSE, or UNVERIFIED and provide a brief justification using both replies and evidence.\"\n",
        "#                 )\n",
        "\n",
        "#             instruction_prompt = f\"\"\"\n",
        "# Claim:\n",
        "# \"{claim}\"\n",
        "\n",
        "# Replies:\n",
        "# {formatted_replies}\n",
        "\n",
        "# Evidence:\n",
        "# {formatted_evidence}\n",
        "\n",
        "# Turnaround detected? {turnaround}\n",
        "\n",
        "# Now, based on the above, classify the claim as one of:\n",
        "# - TRUE\n",
        "# - FALSE\n",
        "# - UNVERIFIED\n",
        "\n",
        "# Return the result in this format:\n",
        "# {{\n",
        "#   \"veracity\": \"...\",\n",
        "#   \"justification\": \"...\"\n",
        "# }}\"\"\".strip()\n",
        "\n",
        "#             full_prompt = {\n",
        "#                 \"system\": system_prompt,\n",
        "#                 \"user\": instruction_prompt\n",
        "#             }\n",
        "\n",
        "#             prompt_texts.append(json.dumps(full_prompt, ensure_ascii=False))\n",
        "\n",
        "#         except Exception as e:\n",
        "#             prompt_texts.append(\"\")\n",
        "\n",
        "#     df[\"filtered_prompt\"] = prompt_texts\n",
        "#     df.to_csv(output_path, index=False, encoding=\"utf-8\", errors=\"replace\")\n",
        "#     print(f\"✅ Filtered prompts saved to: {output_path}\")\n",
        "\n",
        "# # Example usage\n",
        "# build_and_attach_filtered_prompts(\"/content/prompt_inputs_cleaned.csv\")\n"
      ],
      "metadata": {
        "id": "WqsrBSaUJiNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a rumor-verification assistant.\n",
        "\n",
        "INPUTS\n",
        "• CLAIM – always present\n",
        "• USER REPLIES – may be missing, noisy, or conflicting\n",
        "• NEWS EVIDENCE – may be missing, partial, or unreliable\n",
        "\n",
        "LABELS\n",
        "TRUE | FALSE | UNVERIFIED\n",
        "\n",
        "DECISION RULES\n",
        "1 Read NEWS EVIDENCE first.\n",
        "   • If a reputable source in the evidence explicitly confirms → TRUE.\n",
        "   • If a reputable source explicitly refutes  → FALSE.\n",
        "2 If evidence is absent or unclear, inspect USER REPLIES.\n",
        "   • Use them only when a clear majority supports the same conclusion and no credible reply contradicts it.\n",
        "3 If information is weak, contradictory, or irrelevant → UNVERIFIED.\n",
        "4 When uncertain, choose UNVERIFIED.\n",
        "5 Use only the information provided in the input. Do not add external facts or assumptions.\n",
        "\n",
        "OUTPUT (return JSON only)\n",
        "{\n",
        "  \"veracity\": \"TRUE | FALSE | UNVERIFIED\",\n",
        "  \"justification\": \"One concise sentence citing the evidence or reply you relied on.\"\n",
        "}\"\"\"\n",
        "\n",
        "\n",
        "def build_and_attach_filtered_prompts(csv_path, output_path=\"output_with_filtered_prompts_v2.csv\"):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    prompt_texts = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            claim = str(row[\"claim\"])\n",
        "            turnaround = str(row.get(\"turnaround\", \"unknown\"))\n",
        "\n",
        "            replies = ast.literal_eval(row[\"reply_stances_json\"]) if pd.notna(row[\"reply_stances_json\"]) else []\n",
        "            evidence = ast.literal_eval(row[\"top_evidence_sentences\"]) if pd.notna(row[\"top_evidence_sentences\"]) else []\n",
        "\n",
        "            # Format replies\n",
        "            formatted_replies = (\n",
        "                \"\\n\".join(\n",
        "                    f'- \"{r[\"text\"].strip()}\" (Stance: {r.get(\"stance\", \"unknown\")})'\n",
        "                    for r in replies if r.get(\"text\", \"\").strip()\n",
        "                ) or \"No replies provided.\"\n",
        "            )\n",
        "\n",
        "            # Format evidence\n",
        "            formatted_evidence = (\n",
        "                \"\\n\".join(f'- \"{e.strip()}\"' for e in evidence if e.strip()) or \"No evidence provided.\"\n",
        "            )\n",
        "\n",
        "            instruction_prompt = f\"\"\"\n",
        "Claim:\n",
        "\\\"{claim}\\\"\n",
        "\n",
        "Replies:\n",
        "{formatted_replies}\n",
        "\n",
        "Evidence:\n",
        "{formatted_evidence}\n",
        "\n",
        "Turnaround detected? {turnaround}\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "  \"veracity\": \"...\",\n",
        "  \"justification\": \"...\"\n",
        "}}\"\"\".strip()\n",
        "\n",
        "            prompt_texts.append(json.dumps({\"system\": SYSTEM_PROMPT, \"user\": instruction_prompt}, ensure_ascii=False))\n",
        "\n",
        "        except Exception:\n",
        "            prompt_texts.append(\"\")\n",
        "\n",
        "    df[\"filtered_prompt\"] = prompt_texts\n",
        "    df.to_csv(output_path, index=False, encoding=\"utf-8\", errors=\"replace\")\n",
        "    print(f\"✅ Filtered prompts saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "I8F_HccERmUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SYSTEM_PROMPT = (\n",
        "#     \"You are a rumor-verification assistant.\\n\\n\"\n",
        "#     \"INPUTS\\n\"\n",
        "#     \"• CLAIM – always present\\n\"\n",
        "#     \"• REPLIES – may be empty or noisy\\n\"\n",
        "#     \"• NEWS EVIDENCE – may be empty or noisy\\n\\n\"\n",
        "#     \"TASK\\n\"\n",
        "#     \"1. Decide whether the claim is TRUE, FALSE, or UNVERIFIED.\\n\"\n",
        "#     \"2. Base your decision on the most reliable information available:\\n\"\n",
        "#     \"   • If the replies clearly support/refute the claim, use them.\\n\"\n",
        "#     \"   • If the evidence clearly supports/refutes the claim, use it.\\n\"\n",
        "#     \"   • If both sources are credible and relevant, combine them.\\n\"\n",
        "#     \"   • If neither source is relevant or both are missing, label UNVERIFIED.\\n\"\n",
        "#     \"3. Give a concise justification citing the source(s) you relied on.\\n\"\n",
        "#     \"4. Never fabricate facts; if information is insufficient, output UNVERIFIED.\"\n",
        "# )\n",
        "\n",
        "# def build_and_attach_filtered_prompts(csv_path, output_path=\"output_with_filtered_prompts.csv\"):\n",
        "#     df = pd.read_csv(csv_path)\n",
        "#     prompt_texts = []\n",
        "\n",
        "#     for _, row in df.iterrows():\n",
        "#         try:\n",
        "#             claim = str(row[\"claim\"])\n",
        "#             turnaround = str(row.get(\"turnaround\", \"unknown\"))\n",
        "\n",
        "#             replies = ast.literal_eval(row[\"reply_stances_json\"]) if pd.notna(row[\"reply_stances_json\"]) else []\n",
        "#             evidence = ast.literal_eval(row[\"top_evidence_sentences\"]) if pd.notna(row[\"top_evidence_sentences\"]) else []\n",
        "\n",
        "#             # Format replies\n",
        "#             formatted_replies = (\n",
        "#                 \"\\n\".join(\n",
        "#                     f'- \"{r[\"text\"].strip()}\" (Stance: {r.get(\"stance\", \"unknown\")})'\n",
        "#                     for r in replies if r.get(\"text\", \"\").strip()\n",
        "#                 ) or \"No replies provided.\"\n",
        "#             )\n",
        "\n",
        "#             # Format evidence\n",
        "#             formatted_evidence = (\n",
        "#                 \"\\n\".join(f'- \"{e.strip()}\"' for e in evidence if e.strip()) or \"No evidence provided.\"\n",
        "#             )\n",
        "\n",
        "#             instruction_prompt = f\"\"\"\n",
        "# Claim:\n",
        "# \\\"{claim}\\\"\n",
        "\n",
        "# Replies:\n",
        "# {formatted_replies}\n",
        "\n",
        "# Evidence:\n",
        "# {formatted_evidence}\n",
        "\n",
        "# Turnaround detected? {turnaround}\n",
        "\n",
        "# Return JSON:\n",
        "# {{\n",
        "#   \"veracity\": \"...\",\n",
        "#   \"justification\": \"...\"\n",
        "# }}\"\"\".strip()\n",
        "\n",
        "#             prompt_texts.append(json.dumps({\"system\": SYSTEM_PROMPT, \"user\": instruction_prompt}, ensure_ascii=False))\n",
        "\n",
        "#         except Exception:\n",
        "#             prompt_texts.append(\"\")\n",
        "\n",
        "#     df[\"filtered_prompt\"] = prompt_texts\n",
        "#     df.to_csv(output_path, index=False, encoding=\"utf-8\", errors=\"replace\")\n",
        "#     print(f\"✅ Filtered prompts saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "pYnBGEPu3UZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_and_attach_filtered_prompts('/content/prompt_inputs_cleaned.csv')"
      ],
      "metadata": {
        "id": "wW05Spn-jiUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"openai==0.28.1\"\n"
      ],
      "metadata": {
        "id": "VMAi0JLk71qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "Xd9IEK4SWp6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"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\"\n"
      ],
      "metadata": {
        "id": "79THZkNxWoZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import ast\n",
        "import pandas as pd\n",
        "import openai\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "\n",
        "# ------------- CONFIG -------------------------------------------------------\n",
        "\n",
        "CSV_IN  = \"output_with_filtered_prompts_v2.csv\"   # <- produced in previous step\n",
        "CSV_OUT = \"output_with_gpt35_responses_v2.csv\"\n",
        "MODEL   = \"gpt-3.5-turbo-0125\"                 # July-25-2025 default\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "\n",
        "# ------------- BACK-OFF DECORATOR  ------------------------------------------\n",
        "@retry(wait=wait_random_exponential(multiplier=1, max=60), stop=stop_after_attempt(6))\n",
        "def call_chat_completions(prompt_dict: dict) -> str:\n",
        "    \"\"\"\n",
        "    Tries up to 6 times with exponential back-off to cope with transient errors / rate limits.\n",
        "    Returns the assistant message content as a string.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt_dict[\"system\"]},\n",
        "        {\"role\": \"user\",   \"content\": prompt_dict[\"user\"]},\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(model=MODEL, messages=messages)\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# ------------- MAIN ---------------------------------------------------------\n",
        "\n",
        "def attach_gpt35_responses(csv_in: str = CSV_IN, csv_out: str = CSV_OUT) -> None:\n",
        "    df = pd.read_csv(csv_in)\n",
        "\n",
        "    responses = []\n",
        "    for idx, prompt_json in enumerate(df[\"filtered_prompt\"]):\n",
        "        try:\n",
        "            prompt_dict = json.loads(prompt_json)\n",
        "        except json.JSONDecodeError:\n",
        "            # If the previous cell inserted a blank or malformed prompt\n",
        "            responses.append(\"\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            answer = call_chat_completions(prompt_dict)\n",
        "        except Exception as e:\n",
        "            print(f\"[Row {idx}] 🛑 GPT call failed after retries → {e}\")\n",
        "            answer = \"\"\n",
        "\n",
        "        # Optional: validate JSON structure returned by the model\n",
        "        try:\n",
        "            # Expected format:\n",
        "            # {\"veracity\": \"...\", \"justification\": \"...\"}\n",
        "            json.loads(answer)\n",
        "        except json.JSONDecodeError:\n",
        "            # Keep raw text so reviewers can inspect failures\n",
        "            pass\n",
        "\n",
        "        responses.append(answer)\n",
        "        time.sleep(0.2)  # small courtesy delay; adjust for quota\n",
        "\n",
        "    df[\"gpt_response\"] = responses\n",
        "    df.to_csv(csv_out, index=False, encoding=\"utf-8\", errors=\"replace\")\n",
        "    print(f\"✅ GPT-3.5 responses saved to: {csv_out}\")\n",
        "\n",
        "# ------------------ RUN -----------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    attach_gpt35_responses()\n"
      ],
      "metadata": {
        "id": "US4cW0gjkSsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"output_with_gpt35_responses_v2.csv\")"
      ],
      "metadata": {
        "id": "AiYcPr5x7dRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ---------------- CONFIG ------------------\n",
        "CSV_PATH = \"output_with_gpt35_responses_v2.csv\"\n",
        "GOLD_COL = \"gold_label\"     # replace with your actual column name\n",
        "PRED_COL = \"gpt_response\"   # model predictions (JSON with \"veracity\" key)\n",
        "\n",
        "# ---------------- LOAD --------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# ---------------- PARSE PREDICTIONS --------------------\n",
        "def extract_label(response):\n",
        "    try:\n",
        "        parsed = json.loads(response)\n",
        "        return parsed.get(\"veracity\", \"\").strip().upper()  # normalize\n",
        "    except Exception:\n",
        "        return \"INVALID\"\n",
        "\n",
        "df[\"predicted_label\"] = df[PRED_COL].apply(extract_label)\n",
        "df[\"gold_label\"] = df[GOLD_COL].str.upper().str.strip()  # normalize\n",
        "\n",
        "# Optional: filter out rows with parsing errors\n",
        "df = df[df[\"predicted_label\"].isin([\"TRUE\", \"FALSE\", \"UNVERIFIED\"])]\n",
        "df = df[df[\"gold_label\"].isin([\"TRUE\", \"FALSE\", \"UNVERIFIED\"])]\n",
        "\n",
        "# ---------------- REPORT --------------------\n",
        "print(\"✅ Classification Report (GPT vs Gold):\\n\")\n",
        "print(classification_report(\n",
        "    df[\"gold_label\"],\n",
        "    df[\"predicted_label\"],\n",
        "    labels=[\"TRUE\", \"FALSE\", \"UNVERIFIED\"],\n",
        "    digits=3\n",
        "))\n"
      ],
      "metadata": {
        "id": "JWDsfqnDNS5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ---------------- CONFIG ------------------\n",
        "CSV_PATH = \"output_with_gpt35_responses_v2.csv\"\n",
        "GOLD_COL = \"gold_label\"     # adjust to your actual gold column name\n",
        "PRED_COL = \"gpt_response\"\n",
        "\n",
        "# ---------------- LOAD AND EXTRACT ------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "def extract_veracity(response):\n",
        "    try:\n",
        "        return json.loads(response).get(\"veracity\", \"\").strip().upper()\n",
        "    except Exception:\n",
        "        return \"INVALID\"\n",
        "\n",
        "df[\"predicted_label\"] = df[PRED_COL].apply(extract_veracity)\n",
        "df[\"gold_label\"] = df[GOLD_COL].astype(str).str.upper().str.strip()\n",
        "\n",
        "# ---------------- CLEAN ------------------\n",
        "valid_labels = [\"TRUE\", \"FALSE\", \"UNVERIFIED\"]\n",
        "df = df[df[\"predicted_label\"].isin(valid_labels)]\n",
        "df = df[df[\"gold_label\"].isin(valid_labels)]\n",
        "\n",
        "# ---------------- MACRO F1 ------------------\n",
        "macro_f1 = f1_score(\n",
        "    df[\"gold_label\"],\n",
        "    df[\"predicted_label\"],\n",
        "    labels=valid_labels,\n",
        "    average=\"macro\"\n",
        ")\n",
        "\n",
        "print(f\"🎯 Macro F1 Score: {macro_f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "NQrw5wO8O9KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_thread(zip_path, output_dir=None):\n",
        "    \"\"\"\n",
        "    Unzips a .zip file (e.g., thread folder) into a target directory.\n",
        "\n",
        "    Args:\n",
        "        zip_path (str): Path to the .zip file.\n",
        "        output_dir (str): Directory to extract into (optional). If None, extracts to same folder as zip.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to extracted folder\n",
        "    \"\"\"\n",
        "    if output_dir is None:\n",
        "        output_dir = os.path.splitext(zip_path)[0]\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "\n",
        "    print(f\"✅ Extracted: {zip_path} → {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "# Example usage:\n",
        "unzip_thread(\"/content/552783238415265792.zip\")\n"
      ],
      "metadata": {
        "id": "6x6zWhq6O9qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def extract_urls_from_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Scans all .txt files in the folder and returns a list of all URLs found.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing .txt files.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: All non-empty lines (URLs) collected from the files.\n",
        "    \"\"\"\n",
        "    url_list = []\n",
        "\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise FileNotFoundError(f\"❌ Folder not found: {folder_path}\")\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    url = line.strip()\n",
        "                    if url:\n",
        "                        url_list.append(url)\n",
        "\n",
        "    print(f\"✅ Found {len(url_list)} URLs in {folder_path}\")\n",
        "    return url_list\n",
        "\n",
        "# Example usage:\n",
        "folder_path = \"/content/552783238415265792/552783238415265792/google\"\n",
        "urls = extract_urls_from_folder(folder_path)\n",
        "\n",
        "# Preview first 5 URLs\n",
        "print(urls[:5])\n"
      ],
      "metadata": {
        "id": "rxLCc8Ay2idG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Path to your annotation file\n",
        "annotation_path = \"/content/552783238415265792/552783238415265792/annotation.json\"\n",
        "\n",
        "# Load JSON and extract links\n",
        "with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    annotation = json.load(f)\n",
        "\n",
        "# Extract links from the \"links\" field\n",
        "url_list = [item[\"link\"] for item in annotation.get(\"links\", [])]\n",
        "\n",
        "# Show result\n",
        "print(f\"✅ Extracted {len(url_list)} links:\")\n",
        "print(url_list)\n"
      ],
      "metadata": {
        "id": "LDw9L2DH4OT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Load the dataset\n",
        "csv_path = \"output_with_gpt35_responses_v2 (1).csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Define the folder for output\n",
        "output_dir = \"leave_one_out_jsonl\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Ensure the necessary columns exist\n",
        "required_cols = ['event', 'claim', 'gpt_response']\n",
        "if not all(col in df.columns for col in required_cols):\n",
        "    raise ValueError(f\"Missing required columns. Required: {required_cols}\")\n",
        "\n",
        "# Iterate over each unique event\n",
        "for event in df[\"event\"].unique():\n",
        "    # Split data\n",
        "    train_df = df[df[\"event\"] != event]\n",
        "    val_df = df[df[\"event\"] == event]\n",
        "\n",
        "    # Format function for OpenAI JSONL format\n",
        "    def format_rows(sub_df):\n",
        "        formatted = []\n",
        "        for _, row in sub_df.iterrows():\n",
        "            prompt = f\"\"\"You are a rumor verification assistant. Given a claim, some user replies, and external evidence, decide if the claim is TRUE, FALSE, or UNVERIFIED, and explain your reasoning.\\n\\nClaim: {row['claim']}\\n\\nTop Replies:\\n{row.get('reply_stances_json', '')}\\n\\nEvidence:\\n{row.get('top_evidence_sentences', '')}\"\"\"\n",
        "            answer = row[\"gpt_response\"]\n",
        "            formatted.append({\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that verifies claims using replies and evidence.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                    {\"role\": \"assistant\", \"content\": answer}\n",
        "                ]\n",
        "            })\n",
        "        return formatted\n",
        "\n",
        "    # Generate formatted examples\n",
        "    train_data = format_rows(train_df)\n",
        "    val_data = format_rows(val_df)\n",
        "\n",
        "    # Write to .jsonl\n",
        "    train_path = os.path.join(output_dir, f\"train_except_{event}.jsonl\")\n",
        "    val_path = os.path.join(output_dir, f\"val_{event}.jsonl\")\n",
        "\n",
        "    with open(train_path, \"w\") as f:\n",
        "        for item in train_data:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "    with open(val_path, \"w\") as f:\n",
        "        for item in val_data:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(f\"✅ LOEO fine-tuning files saved to: {output_dir}/\")\n"
      ],
      "metadata": {
        "id": "OYb0ZYO-44rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"output_with_gpt35_responses_v2 (1).csv\")\n",
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "qbazxT4NOq6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai\n"
      ],
      "metadata": {
        "id": "MkaWuHd-W5M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd leave_one_out_jsonl\n"
      ],
      "metadata": {
        "id": "gX1nLXhCOrmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --version\n"
      ],
      "metadata": {
        "id": "EuAjAm-zXubv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai files upload --purpose fine-tune --file train_except_charliehebdo-all-rnr-threads.jsonl\n",
        "!openai files upload --purpose fine-tune --file val_charliehebdo-all-rnr-threads.jsonl\n"
      ],
      "metadata": {
        "id": "aKPYuNSgWU5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load CSV file\n",
        "csv_path = \"/content/output_with_gpt35_responses_v2 (1).csv\"  # Adjust path if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_cols = {\"event\", \"filtered_prompt\", \"gpt_response\"}\n",
        "if not required_cols.issubset(df.columns):\n",
        "    raise ValueError(f\"Missing one of required columns: {required_cols}\")\n",
        "\n",
        "# Output directory\n",
        "output_dir = \"jsonl_loeo\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Group by events\n",
        "events = df[\"event\"].unique()\n",
        "\n",
        "for event in events:\n",
        "    # Leave-one-event-out split\n",
        "    train_df = df[df[\"event\"] != event]\n",
        "    test_df = df[df[\"event\"] == event]\n",
        "\n",
        "    # Format for OpenAI fine-tuning JSONL\n",
        "    def format_row(row):\n",
        "        return {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a rumor verification assistant. You will receive a claim, replies from users, and supporting news evidence. Classify the claim as TRUE, FALSE, or UNVERIFIED and provide a brief justification.\"},\n",
        "                {\"role\": \"user\", \"content\": str(row[\"filtered_prompt\"])},\n",
        "                {\"role\": \"assistant\", \"content\": str(row[\"gpt_response\"])}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    # Convert to JSONL format\n",
        "    train_jsonl = [format_row(row) for _, row in train_df.iterrows()]\n",
        "    test_jsonl = [format_row(row) for _, row in test_df.iterrows()]\n",
        "\n",
        "    # Save to files\n",
        "    train_path = os.path.join(output_dir, f\"train_leaveout_{event}.jsonl\")\n",
        "    test_path = os.path.join(output_dir, f\"test_leaveout_{event}.jsonl\")\n",
        "\n",
        "    with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in train_jsonl:\n",
        "            f.write(json.dumps(line) + \"\\n\")\n",
        "\n",
        "    with open(test_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in test_jsonl:\n",
        "            f.write(json.dumps(line) + \"\\n\")\n",
        "\n",
        "print(f\"JSONL files saved in: {output_dir}\")\n"
      ],
      "metadata": {
        "id": "iUPIVOdTWeiF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}